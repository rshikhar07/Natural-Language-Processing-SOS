{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4221247e-e251-46d8-bba6-9f7f8ab246c5",
   "metadata": {},
   "source": [
    "# DAY1 (19.5.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc98c5d-4d8a-4d6e-8833-ca2e641b4694",
   "metadata": {},
   "source": [
    "# What is Natural Language Processing (NLP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfc211-1d8a-4bf9-940b-2a03b509550c",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a subfield of Artificial Intelligence (AI) and Computational Linguistics that enables machines to process, analyze, and generate human (natural) language. It involves a combination of linguistics, machine learning, and statistical methods to interpret text or speech data in a meaningful way.\n",
    "\n",
    "NLP tasks typically involve transforming raw, unstructured language data into structured representations suitable for downstream machine learning tasks. This often includes converting text into numeric feature vectors, identifying linguistic structures (syntax, semantics), and applying probabilistic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065ad85-c7a6-4d00-821e-e79cc97c96ee",
   "metadata": {},
   "source": [
    "##  Core Applications and Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95735e98-d195-4b95-b3e0-1a71daca9d12",
   "metadata": {},
   "source": [
    "1. Text Classification\n",
    "Assigning predefined categories to text documents (e.g., spam detection, sentiment analysis, topic classification) using supervised models like Naive Bayes, SVMs, or deep learning models.\n",
    "\n",
    "2. Tokenization & POS Tagging\n",
    "Breaking text into meaningful units (tokens) and tagging each word with its part of speech using models like Hidden Markov Models (HMMs) or neural   networks.\n",
    "\n",
    "3. Named Entity Recognition (NER)\n",
    "Identifying and classifying named entities (like persons, organizations, locations) using sequence models such as CRFs, BiLSTM-CRF, or Transformers (e.g., BERT-based NER).\n",
    "\n",
    "4. Language Modeling & Text Generation\n",
    "Modeling the probability distribution of word sequences for applications like autocomplete, speech synthesis, and machine translation. Common approaches include n-grams, RNNs, LSTMs, and Transformers.\n",
    "\n",
    "5. Machine Translation\n",
    "Translating text from one language to another using seq2seq models with attention mechanisms or Transformer-based architectures (e.g., Google’s T5, OpenNMT).\n",
    "\n",
    "6. Information Retrieval & Question Answering\n",
    "Retrieving relevant information or direct answers from large corpora using techniques like TF-IDF, BM25, or dense retrieval models like DPR and BERT-based QA systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35fc3a4-13c1-4c3d-a0a4-791b5ea7eb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK and spaCy imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "print(\"NLTK and spaCy imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89873f7e-8d1b-4b3c-af7e-39453e1320a5",
   "metadata": {},
   "source": [
    "# Some NLP Concepts\n",
    "\n",
    "## WordNet\n",
    "WordNet is a large lexical database of English.\n",
    "Words are grouped into synsets (sets of cognitive synonyms).\n",
    "It helps capture semantic relationships such as:\n",
    "Synonyms (words with similar meanings)\n",
    "Hypernyms (general terms)\n",
    "Hyponyms (specific terms)\n",
    "Antonyms\n",
    "Used in lemmatization and meaning-based NLP tasks.\n",
    "\n",
    "## One-Hot Encoding\n",
    "Represents each word in the vocabulary as a binary vector.\n",
    "All values are 0 except for a single 1 at the index corresponding to that word.\n",
    "Example (vocab = [“apple”, “banana”, “car”]):\n",
    "\"banana\" → [0, 1, 0]\n",
    "Limitations:\n",
    "High-dimensional (for large vocabularies)\n",
    "No information about similarity or meaning (all vectors are orthogonal)\n",
    "\n",
    "## Word Vectors\n",
    "Word vectors (a.k.a. word embeddings) represent words as dense, continuous-valued vectors.\n",
    "Unlike one-hot vectors, word vectors capture semantic similarity.\n",
    "Words with similar meaning lie closer in the vector space.\n",
    "Example:\n",
    "vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")\n",
    "\n",
    "## Word2Vec\n",
    "A popular algorithm to learn word embeddings using shallow neural networks.\n",
    "Two main architectures:\n",
    "CBOW (Continuous Bag of Words) – Predicts the target word from surrounding context.\n",
    "Skip-Gram – Predicts context words given the target word.\n",
    "Learns from co-occurrence patterns in large corpora.\n",
    "Produces meaningful vector relationships (semantic arithmetic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b761b4d1-734b-4b29-9ec8-c02d83c6fdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Asus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Asus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Asus/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec88e45-9008-4485-90e2-64046ab1b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets of 'good': [Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n",
      "Definition: benefit\n",
      "Examples: ['for your own good', \"what's the good of worrying?\"]\n",
      "Definition: moral excellence or admirableness\n",
      "Examples: ['there is much good to be found in people']\n",
      "Definition: that which is pleasing or valuable or useful\n",
      "Examples: ['weigh the good against the bad', 'among the highest goods of all are happiness and self-realization']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Get synsets for a word\n",
    "synsets = wn.synsets(\"good\")\n",
    "print(\"Synsets of 'good':\", synsets)\n",
    "\n",
    "# Explore definitions and examples\n",
    "for syn in synsets[:3]:\n",
    "    print(f\"Definition: {syn.definition()}\")\n",
    "    print(f\"Examples: {syn.examples()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd0e62-7129-4654-b04c-b9305b0bbbea",
   "metadata": {},
   "source": [
    "# DAY 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd744581-c1e0-424a-ac4f-ff2cc2f817b5",
   "metadata": {},
   "source": [
    "##  Lecture 2 Summary – CS224N: NLP with Deep Learning\n",
    "\n",
    "###  1. Optimization Basics\n",
    "- Introduction to gradient descent for minimizing loss functions in NLP.\n",
    "- Importance of objective functions in training embeddings like word2vec.\n",
    "\n",
    "###  2. Word2Vec Review\n",
    "- Two architectures: **CBOW (Continuous Bag of Words)** and **Skip-gram**.\n",
    "- Trains word embeddings based on local context in a window.\n",
    "- Optimization through **negative sampling** or **hierarchical softmax**.\n",
    "\n",
    "###  3. Word Vectors\n",
    "- Dense vector representations of words capturing semantic similarity.\n",
    "- Words like `[\"king\", \"queen\", \"man\", \"woman\"]` form analogies using vector arithmetic.\n",
    "\n",
    "###  4. Capturing Word Meaning via Counting\n",
    "- Co-occurrence matrix from large corpora (e.g., how often words appear together).\n",
    "- **PMI (Pointwise Mutual Information)** and **SVD (Singular Value Decomposition)** used for dimensionality reduction.\n",
    "- Count-based models vs prediction-based models (like word2vec).\n",
    "\n",
    "###  5. Evaluating Word Embeddings\n",
    "- Intrinsic tasks: similarity (e.g., cosine similarity), analogy tasks.\n",
    "- Extrinsic tasks: downstream NLP tasks like NER or POS tagging.\n",
    "\n",
    "###  6. Word Senses\n",
    "- Polysemy issue: words have multiple meanings.\n",
    "- Embeddings trained on full corpora struggle with disambiguation.\n",
    "\n",
    "###  7. Neural Network Classifiers\n",
    "- Neural networks for NLP: feedforward and shallow architectures.\n",
    "- Layers: input → hidden → output (softmax).\n",
    "\n",
    "---\n",
    "\n",
    " **Key Goal:** Understand how word embeddings are generated and evaluated, and how neural networks begin to classify based on learned features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9ac53-9bf4-411e-a7e1-b8e361998fe6",
   "metadata": {},
   "source": [
    "#  NLP Pipeline: From Text to Features\n",
    "\n",
    "The **Natural Language Processing (NLP) pipeline** is a sequence of steps that transforms raw text into features that can be fed into machine learning or deep learning models.\n",
    "\n",
    "###  Common NLP Pipeline Steps:\n",
    "1. **Raw Text**: Input text from articles, social media, etc.\n",
    "2. **Tokenization**: Split the text into words, punctuation, or subwords.\n",
    "3. **Text Cleaning**: Lowercasing, removing stopwords, stemming, lemmatization.\n",
    "4. **Feature Extraction**: Convert text to numbers using:\n",
    "   - Bag-of-Words\n",
    "   - TF-IDF\n",
    "   - Word Embeddings (Word2Vec, GloVe, etc.)\n",
    "5. **Modeling**: Use features as input for classification, translation, etc.\n",
    "\n",
    " Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1ac50e-3926-44cf-a7c0-77d12213c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Natural Language Processing (NLP) enables machines to understand human language.\n",
      "\n",
      "Tokens:\n",
      "['Natural', 'Language', 'Processing', '(NLP)', 'enables', 'machines', 'to', 'understand', 'human', 'language.']\n"
     ]
    }
   ],
   "source": [
    "# Load sample text\n",
    "text = \"Natural Language Processing (NLP) enables machines to understand human language.\"\n",
    "\n",
    "# Print the original text\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "\n",
    "# Split into words\n",
    "tokens = text.split()\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb93a85-1264-4a75-b9d8-ec83e9308617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Asus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to C:\\Users\\Asus/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')      # Lemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagging\n",
    "nltk.download('brown')        # Sample corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aab6528-2960-4628-aadb-020ec6d26797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "Sentence 2: ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "Sentence 3: ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n",
      "Sentence 4: ['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.']\n",
      "Sentence 5: ['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"Georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# Get first few sentences from the Brown corpus\n",
    "sentences = brown.sents()[:5]\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1089a45-360c-4d19-8733-d037a825df70",
   "metadata": {},
   "source": [
    "# DAY 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9437ad-ce39-495c-993f-3df3c3ef7b15",
   "metadata": {},
   "source": [
    "#  CS224N Lecture Summary: Neural Network Foundations in NLP\n",
    "\n",
    "##  Objective\n",
    "To understand the **mathematical foundations** of how neural networks learn in the context of NLP tasks, with a focus on:\n",
    "- Gradient computation\n",
    "- Backpropagation\n",
    "- Named Entity Recognition (NER)\n",
    "- Matrix calculus\n",
    "- Automatic differentiation in modern frameworks like PyTorch\n",
    "\n",
    "##  Key Concepts Covered\n",
    "\n",
    "###  1. **Gradient Computation**\n",
    "- Gradients are crucial for training neural networks.\n",
    "- They indicate the direction and rate at which model parameters should change to minimize the loss function.\n",
    "- The process of computing these gradients relies on calculus and linear algebra.\n",
    "\n",
    "###  2. **Backpropagation Algorithm**\n",
    "- A core algorithm that efficiently computes gradients using the chain rule of calculus.\n",
    "- It involves:\n",
    "  - **Forward pass:** computing predictions using current model weights.\n",
    "  - **Backward pass:** calculating the gradient of the loss with respect to each parameter.\n",
    "\n",
    ">  Backpropagation allows each layer to reuse gradients, making training scalable even in deep networks.\n",
    "\n",
    "###  3. **Named Entity Recognition (NER)**\n",
    "- Used as a real-world example to apply neural network principles.\n",
    "- Highlights the importance of **context** in classifying entities like:\n",
    "  - People’s names\n",
    "  - Locations\n",
    "  - Organizations\n",
    "- A simple model: concatenate word vectors from a window of words → pass through a neural network → classify.\n",
    "\n",
    "###  4. **Matrix Calculus & Vectorization**\n",
    "- Working with **vectors and matrices** is more efficient than scalar-by-scalar computations.\n",
    "- Neural networks rely heavily on **vectorized gradient computation** using matrix calculus.\n",
    "\n",
    ">  Enables faster computation and easier optimization in frameworks like PyTorch and TensorFlow.\n",
    "\n",
    "###  5. **Automatic Differentiation**\n",
    "- A powerful tool offered by modern libraries (e.g., PyTorch, TensorFlow).\n",
    "- Automatically constructs the **computation graph** and calculates derivatives.\n",
    "- Saves time and reduces the likelihood of errors in gradient implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9266c4-7059-432d-bbac-95c389f822e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 1.0/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 1.0/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.5 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.3 MB/s eta 0:00:08\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 1.4 MB/s eta 0:00:07\n",
      "     ------------- -------------------------- 4.2/12.8 MB 1.5 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 4.7/12.8 MB 1.6 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.7 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 6.0/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.8 MB 1.8 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.1/12.8 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.9/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.9 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c82ffc-271e-4e93-a2c2-237c3fe24ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3499947b-2a05-405f-8dd1-14ecc1f50fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Apple is looking at buying U.K. startup for $1 billion. This news was published yesterday.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccae70e6-a266-49ac-8157-a94166f7d4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Sentence Tokenization:\n",
      "['Apple is looking at buying U.K. startup for $1 billion.', 'This news was published yesterday.']\n"
     ]
    }
   ],
   "source": [
    "nltk_sentences = sent_tokenize(sample_text)\n",
    "print(\"NLTK Sentence Tokenization:\")\n",
    "print(nltk_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33fcba88-8d95-4594-b431-b1e97c071509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Word Tokenization:\n",
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'This', 'news', 'was', 'published', 'yesterday', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk_words = word_tokenize(sample_text)\n",
    "print(\"NLTK Word Tokenization:\")\n",
    "print(nltk_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d486969-dd90-465f-bea3-497e81158d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Sentence Tokenization:\n",
      "Apple is looking at buying U.K. startup for $1 billion.\n",
      "This news was published yesterday.\n",
      "\n",
      "spaCy Word Tokenization:\n",
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n",
      ".\n",
      "This\n",
      "news\n",
      "was\n",
      "published\n",
      "yesterday\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sample_text)\n",
    "\n",
    "# Sentence tokenization\n",
    "print(\"spaCy Sentence Tokenization:\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "\n",
    "# Word tokenization\n",
    "print(\"\\nspaCy Word Tokenization:\")\n",
    "for token in doc:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f142a902-a6fe-4ca4-b4de-d47254c8a156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens: ['Elon', 'Musk', 'founded', 'SpaceX', 'in', '2002', '.']\n",
      "spaCy Tokens: ['Elon', 'Musk', 'founded', 'SpaceX', 'in', '2002', '.']\n"
     ]
    }
   ],
   "source": [
    "def nltk_tokenizer(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Test on a custom input\n",
    "test_text = \"Elon Musk founded SpaceX in 2002.\"\n",
    "print(\"NLTK Tokens:\", nltk_tokenizer(test_text))\n",
    "print(\"spaCy Tokens:\", spacy_tokenizer(test_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9a933-fa16-4760-8fa8-eea12cb6414e",
   "metadata": {},
   "source": [
    "###  Comparison: NLTK vs spaCy Tokenization\n",
    "\n",
    "| Feature                  | NLTK                         | spaCy                        |\n",
    "|--------------------------|------------------------------|------------------------------|\n",
    "| Sentence Tokenizer       | Uses pre-trained Punkt model | Built-in, context-aware      |\n",
    "| Word Tokenizer           | Rule-based                   | Rule-based + linguistic info |\n",
    "| Named Entity Detection   | Not built-in                 | Built-in                   |\n",
    "| Speed                    | Fast                         | Faster + efficient           |\n",
    "\n",
    ">  **spaCy** offers richer linguistic features and better integration with NLP pipelines, while **NLTK** is great for learning and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6ef2c-300a-430d-ba5a-362d50e84828",
   "metadata": {},
   "source": [
    "#  Day 4 – May 22: Dependency Parsing\n",
    "\n",
    "##  Topic Overview\n",
    "\n",
    "Today’s lecture focused on **Dependency Parsing**, a core technique in natural language processing (NLP) that helps analyze the grammatical structure of a sentence. It identifies relationships between words (called **dependencies**) and forms a syntactic structure that machines can understand. Unlike constituency parsing which focuses on phrases, dependency parsing is **word-to-word relational**.\n",
    "\n",
    "##  Key Learnings\n",
    "\n",
    "###  What is Dependency Parsing?\n",
    "\n",
    "- In **dependency grammar**, a sentence is structured around binary relationships between words.\n",
    "- Each relationship is a directed link from a **head** to a **dependent**.\n",
    "\n",
    "####  Example:\n",
    "Sentence: `\"The quick brown fox jumps over the lazy dog.\"`\n",
    "\n",
    "Dependency Relations:\n",
    "- \"jumps\" → head of the sentence (root verb)\n",
    "- \"fox\" → subject of \"jumps\"\n",
    "- \"quick\", \"brown\" → modifiers of \"fox\"\n",
    "- \"over\" → preposition linked to \"jumps\"\n",
    "- \"dog\" → object of \"over\"\n",
    "- \"lazy\" → modifier of \"dog\"\n",
    "\n",
    "###  Syntax and Sentence Structure\n",
    "\n",
    "- Words have **parts of speech (POS)** (e.g., noun, verb, adjective), and their arrangement determines sentence meaning.\n",
    "- Syntactic structure helps in tasks like translation, question answering, and sentiment analysis.\n",
    "\n",
    "####  Example:\n",
    "- `\"Alice gave Bob a book.\"`\n",
    "  - Subject: Alice\n",
    "  - Verb: gave\n",
    "  - Indirect Object: Bob\n",
    "  - Direct Object: book\n",
    "\n",
    "The relationships here form a tree with \"gave\" as the root.\n",
    "\n",
    "###  Universal Dependencies (UD)\n",
    "\n",
    "- UD is a cross-lingual framework for syntactic annotation.\n",
    "- It standardizes dependency labels like:\n",
    "  - `nsubj` (nominal subject)\n",
    "  - `obj` (object)\n",
    "  - `amod` (adjectival modifier)\n",
    "  - `root` (main predicate)\n",
    "\n",
    "####  Example:\n",
    "For `\"She reads a book.\"`, the UD structure would look like:\n",
    "- reads → root\n",
    "- She → nsubj\n",
    "- book → obj\n",
    "- a → det\n",
    "\n",
    "###  Ambiguity in Parsing\n",
    "\n",
    "Language is inherently ambiguous. The same sentence can be parsed in multiple ways based on context.\n",
    "\n",
    "####  Example:\n",
    "Headline: `\"Kids make nutritious snacks\"`\n",
    "- Parse 1: Kids are preparing healthy snacks.\n",
    "- Parse 2: Kids are the snacks.\n",
    "\n",
    "This kind of ambiguity makes dependency parsing both challenging and critical.\n",
    "\n",
    "###  Transition-Based Parsing\n",
    "\n",
    "- A **transition-based parser** builds a dependency tree incrementally using operations:\n",
    "  - `SHIFT`: Move word from buffer to stack.\n",
    "  - `LEFT-ARC`/`RIGHT-ARC`: Create a dependency relation.\n",
    "  - `REDUCE`: Remove a word from the stack.\n",
    "\n",
    "This is modeled as a state machine with actions predicted by a trained classifier.\n",
    "\n",
    "###  Evaluating Parsers\n",
    "\n",
    "- **Treebanks** are annotated datasets used to train and evaluate dependency parsers.\n",
    "- Common metrics:\n",
    "  - **UAS** (Unlabeled Attachment Score): % of correct head assignments.\n",
    "  - **LAS** (Labeled Attachment Score): % of correct head + label assignments.\n",
    "\n",
    "\n",
    "##  Summary\n",
    "\n",
    "- Dependency parsing extracts **\"who modifies whom\"** in a sentence.\n",
    "- Transition-based methods offer an efficient way to perform parsing.\n",
    "- The structural outputs are essential for downstream NLP applications.\n",
    "- Modern models use machine learning to automate parsing decisions based on large treebank datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1844be3a-f8dd-4b38-938b-5a14921ab4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5777159-ac12-43b0-9b40-f97c676dfa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Hello there! This is Day 4 of NLP. We're learning preprocessing – lowercasing, punctuation removal, and tokenization.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b172dd10-de83-4e10-af2d-2b427544fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased Text:\n",
      " hello there! this is day 4 of nlp. we're learning preprocessing – lowercasing, punctuation removal, and tokenization.\n"
     ]
    }
   ],
   "source": [
    "lower_text = sample_text.lower()\n",
    "print(\"Lowercased Text:\\n\", lower_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71db27cd-b702-4594-aef5-c4e5a64d87d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without Punctuation:\n",
      " hello there this is day 4 of nlp were learning preprocessing – lowercasing punctuation removal and tokenization\n"
     ]
    }
   ],
   "source": [
    "# Create a regex pattern for punctuation\n",
    "pattern = f\"[{re.escape(string.punctuation)}]\"\n",
    "no_punct_text = re.sub(pattern, \"\", lower_text)\n",
    "print(\"Text without Punctuation:\\n\", no_punct_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5403e496-b6de-4bbe-b44f-e05bc6f647c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text:\n",
      " ['hello', 'there', 'this', 'is', 'day', '4', 'of', 'nlp', 'were', 'learning', 'preprocessing', '–', 'lowercasing', 'punctuation', 'removal', 'and', 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(no_punct_text)\n",
    "print(\"Tokenized Text:\\n\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ecd5649-f450-47aa-bd53-1fef1707e759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Output:\n",
      " ['hello', 'there', 'this', 'is', 'day', '4', 'of', 'nlp', 'were', 'learning', 'preprocessing', '–', 'lowercasing', 'punctuation', 'removal', 'and', 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Try it on the sample\n",
    "processed = preprocess_text(sample_text)\n",
    "print(\"Preprocessed Output:\\n\", processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ef215-b5d0-4a6c-ab1d-70f49e90f862",
   "metadata": {},
   "source": [
    "#  Day 4 – Text Preprocessing\n",
    "\n",
    "This notebook demonstrates basic preprocessing steps commonly used in NLP:\n",
    "\n",
    "###  Steps Covered:\n",
    "- **Lowercasing** – Standardizes all words to lowercase.\n",
    "- **Punctuation Removal** – Removes all punctuation using `string.punctuation` and regex.\n",
    "- **Tokenization** – Splits the cleaned text into tokens (words).\n",
    "- **Preprocessing Function** – A clean function to perform all steps on any input text.\n",
    "\n",
    "These steps help reduce noise and standardize input data before feeding it into NLP models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665d24b-8881-453a-b949-e45b67256e4c",
   "metadata": {},
   "source": [
    "# DAY 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6337836-7aa3-4210-be46-48f673bfd080",
   "metadata": {},
   "source": [
    "# Neural Dependency Parsing and RNNs \n",
    "\n",
    "## Summary\n",
    "\n",
    "This lecture covers the transition from traditional symbolic parsing methods to neural dependency parsing, discussing how dense, learned representations enhance both performance and speed. It also introduces the basics of neural networks, language modeling, and Recurrent Neural Networks (RNNs), laying the foundation for handling sequential data in NLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bad1e6-a75b-4047-89ac-c64a482f9d61",
   "metadata": {},
   "source": [
    "## 1. Neural Dependency Parsing\n",
    "\n",
    "Traditional parsers rely on hand-crafted, sparse features and symbolic rules, which can be inefficient and limited in generalization.\n",
    "\n",
    "### Example:\n",
    "In symbolic parsing, the model might follow a rule like:\n",
    "- *If the top of the stack is a noun and the first buffer word is a verb, make a dependency link.*\n",
    "\n",
    "However, symbolic models:\n",
    "- Miss nuanced relationships\n",
    "- Require manual feature engineering\n",
    "- Struggle with unseen inputs\n",
    "\n",
    "**Neural Approach:**\n",
    "Neural dependency parsers learn **dense embeddings** for:\n",
    "- Words\n",
    "- POS tags\n",
    "- Dependency labels\n",
    "\n",
    "These are concatenated and fed into a **feedforward neural network**, which predicts the next parsing action (e.g., shift, left-arc, right-arc).\n",
    "\n",
    "This leads to:\n",
    "- Better generalization\n",
    "- Vectorized (faster) computation\n",
    "- No need for manual features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7905e-7544-4be8-9720-18b29019263b",
   "metadata": {},
   "source": [
    "## 2. Advantages of Neural Approaches\n",
    "\n",
    "- Dense embeddings represent semantic similarity (e.g., \"run\" and \"sprint\" are nearby in vector space).\n",
    "- Neural classifiers allow **non-linear** decision boundaries using activations like ReLU or tanh.\n",
    "- Neural models are faster and more accurate due to automatic feature learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a646f3-2e39-4801-bdd3-23d236cd7347",
   "metadata": {},
   "source": [
    "## 3. Regularization and Optimization in Neural Networks\n",
    "\n",
    "To prevent overfitting, neural networks use:\n",
    "\n",
    "### - L2 Regularization:\n",
    "Adds a penalty to large weights during training.\n",
    "\n",
    "### - Dropout:\n",
    "Randomly disables neurons during training to reduce co-adaptation.\n",
    "\n",
    "### Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a8e9499-be2f-4e4c-93ca-04f977883109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # Dropout layer\n",
    "    nn.Linear(50, 10)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff383d9-782d-4fd8-bcf7-6a33d971211e",
   "metadata": {},
   "source": [
    "## 4. Language Modeling\n",
    "\n",
    "Language modeling is the task of predicting the **next word** in a sequence.\n",
    "\n",
    "### Example:\n",
    "Given input: `\"I want to eat a ___\"`  \n",
    "The goal is to predict: `\"pizza\"`\n",
    "\n",
    "Applications:\n",
    "- Predictive text\n",
    "- Speech recognition\n",
    "- Machine translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5886d7-a07f-41cf-aeba-b74aa2c9183b",
   "metadata": {},
   "source": [
    "## 5. N-Gram Language Models\n",
    "\n",
    "These models estimate probabilities using frequency counts of word sequences.\n",
    "\n",
    "### Example:\n",
    "Bigram probability:  \n",
    "P(eat | to) = count(\"to eat\") / count(\"to\")\n",
    "\n",
    "**Limitations:**\n",
    "- Context is fixed to n-1 words\n",
    "- Suffers from data sparsity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da489b2b-e897-4645-a21c-4dfa6fedefce",
   "metadata": {},
   "source": [
    "## 6. Neural Language Models (Window-based)\n",
    "\n",
    "These models embed words and pass them through a neural network to predict the next word.\n",
    "\n",
    "### Example:\n",
    "Input context: `\"I want to eat\"`  \n",
    "→ Convert to embeddings  \n",
    "→ Concatenate  \n",
    "→ Pass through MLP  \n",
    "→ Output probabilities for next word\n",
    "\n",
    "**Advantages:**\n",
    "- Embeddings generalize across similar words\n",
    "- Avoids sparsity problems in n-gram models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f3a1e-af41-4520-8b80-caf8040978c2",
   "metadata": {},
   "source": [
    "## 7. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs allow processing of **sequential data** by maintaining a **hidden state** over time.\n",
    "\n",
    "### Formula:\n",
    "At each time step *t*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff55db44-7343-41dc-adcb-0ac40488e952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# A simple RNN layer\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, batch_first=True)\n",
    "\n",
    "# Input: batch_size x seq_len x input_size\n",
    "x = torch.randn(5, 3, 10)\n",
    "h0 = torch.zeros(1, 5, 20)  # Initial hidden state\n",
    "\n",
    "out, hn = rnn(x, h0)\n",
    "print(out.shape)  # (batch_size, seq_len, hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e0d05-34fe-463e-9063-dc58b74adac4",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "- Learns from arbitrary-length sequences\n",
    "- Shared weights across time steps\n",
    "- Captures long-term dependencies (up to a point)\n",
    "\n",
    "### Limitations:\n",
    "- Sequential computation (slow training)\n",
    "- Struggles with long-range dependencies (solved by LSTMs/GRUs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16daa5e-aa36-485b-9896-ea171f209985",
   "metadata": {},
   "source": [
    "#  Day 5 – Stopword Removal with NLTK (task2)\n",
    "\n",
    "**Date:** May 23  \n",
    "**Topic:** Stopword Removal  \n",
    "**Goal:** Remove common words (“stopwords”) using NLTK’s built-in list, customize the list, and compare before/after.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4edd2a84-0198-4091-953f-2fccdef5f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc33a22-7cce-4ff3-a545-3ae5c5cbd81a",
   "metadata": {},
   "source": [
    "##  Example Text & Tokenization\n",
    "\n",
    "We’ll start with a sample sentence and tokenize it into words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e95446ec-bf57-4752-bc18-abba7ecb5c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:\n",
      "['This', 'is', 'an', 'example', 'sentence', ',', 'demonstrating', 'stopword', 'removal', 'using', 'NLTK', \"'s\", 'default', 'list', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is an example sentence, demonstrating stopword removal using NLTK's default list.\"\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(\"Original Tokens:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d032c8-aa65-46ba-b6fc-581ebe296b02",
   "metadata": {},
   "source": [
    "##  Default Stopword Removal\n",
    "\n",
    "Use NLTK’s English stopword list to filter out common words, and also remove punctuation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dfe8241-7826-4af7-9fec-7b0a51b3573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Default Stopword Removal:\n",
      "['example', 'sentence', 'demonstrating', 'stopword', 'removal', 'using', 'NLTK', \"'s\", 'default', 'list']\n"
     ]
    }
   ],
   "source": [
    "# Load default English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter tokens\n",
    "filtered_default = [\n",
    "    tok for tok in tokens\n",
    "    if tok.lower() not in stop_words and tok not in string.punctuation\n",
    "]\n",
    "\n",
    "print(\"\\nAfter Default Stopword Removal:\")\n",
    "print(filtered_default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bebdb2-76ac-4450-bd7d-85b0cef9d77f",
   "metadata": {},
   "source": [
    "##  Customizing the Stopword List\n",
    "\n",
    "You can **add** domain-specific words or **remove** words you want to keep.  \n",
    "Below, we remove \"this\" from the list and add \"example\" as a stopword.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3787337-8794-423f-adf2-9b76ee1ac941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Custom Stopword Removal:\n",
      "['This', 'sentence', 'demonstrating', 'stopword', 'removal', 'using', 'NLTK', \"'s\", 'default', 'list']\n"
     ]
    }
   ],
   "source": [
    "# Copy and customize\n",
    "custom_sw = stop_words.copy()\n",
    "custom_sw.discard('this')      # Keep “this”\n",
    "custom_sw.add('example')       # Remove “example”\n",
    "\n",
    "# Apply custom list\n",
    "filtered_custom = [\n",
    "    tok for tok in tokens\n",
    "    if tok.lower() not in custom_sw and tok not in string.punctuation\n",
    "]\n",
    "\n",
    "print(\"\\nAfter Custom Stopword Removal:\")\n",
    "print(filtered_custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96cb5f2-4dad-4bf3-a548-a27c791aff01",
   "metadata": {},
   "source": [
    "# Day 6: Recurrent Neural Networks (RNNs) and LSTM Networks in NLP (task1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c011b-04fa-4f60-b64e-8993e4caec24",
   "metadata": {},
   "source": [
    "## Introduction to RNNs and Language Models\n",
    "\n",
    "In natural language processing (NLP), one of the fundamental tasks is language modeling — predicting the next word in a sequence.\n",
    "\n",
    "Traditional models like n-grams are limited by context size. Recurrent Neural Networks (RNNs) are designed to address this by maintaining a *hidden state* that captures information from previous words in a sequence.\n",
    "\n",
    "### Example:\n",
    "Given the sequence:\n",
    "\n",
    "`I am going to the`\n",
    "\n",
    "An RNN tries to predict the next word (like \"store\", \"park\", etc.) using both the current input and hidden state from prior words.\n",
    "\n",
    "## Structure of a Simple RNN\n",
    "\n",
    "An RNN processes sequences one step at a time. At each time step `t`, it receives input `x_t` and previous hidden state `h_{t-1}`, then updates the hidden state and produces an output.\n",
    "\n",
    "### Example Formula:\n",
    "\n",
    "h_t = tanh(W_h * h_{t-1} + W_x * x_t + b)\n",
    "y_t = softmax(W_y * h_t + c)\n",
    "\n",
    "Here, `W_h`, `W_x`, and `W_y` are weight matrices, and `tanh` introduces non-linearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45612bc7-c840-423c-8035-029fc17f2366",
   "metadata": {},
   "source": [
    "## Training RNNs\n",
    "\n",
    "RNNs are trained using a corpus of text. The idea is to show the model a sequence of words and ask it to predict the next word at each step. The difference between the prediction and the actual word is measured using **cross-entropy loss**.\n",
    "\n",
    "Training is typically done using **stochastic gradient descent (SGD)** and **backpropagation through time (BPTT)**.\n",
    "\n",
    "### Example:\n",
    "\n",
    "For the sentence \"The cat sat on the mat\":\n",
    "\n",
    "- Input: \"The\" → Target: \"cat\"\n",
    "- Input: \"The cat\" → Target: \"sat\"\n",
    "- ...\n",
    "\n",
    "## Challenges in RNNs: Vanishing Gradients\n",
    "\n",
    "A major issue in simple RNNs is **vanishing gradients** — gradients become extremely small when propagated back through many time steps. This makes it difficult for RNNs to learn long-term dependencies.\n",
    "\n",
    "### Illustration:\n",
    "In a long sentence, the information at the beginning becomes \"washed out\" as the RNN progresses.\n",
    "\n",
    "Example: \"The movie was **not** good\" — the model may forget \"not\" by the time it processes \"good\".\n",
    "\n",
    "## LSTMs: Solving the Vanishing Gradient Problem\n",
    "\n",
    "**Long Short-Term Memory (LSTM)** networks are a special kind of RNN designed to combat vanishing gradients.\n",
    "\n",
    "LSTMs introduce **gates**:\n",
    "- **Forget Gate**: Decides what information to discard.\n",
    "- **Input Gate**: Determines what new information to store.\n",
    "- **Output Gate**: Controls the output and what passes to the next hidden state.\n",
    "\n",
    "### Example:\n",
    "\n",
    "In the sentence:  \n",
    "\"I lived in France for two years so I speak French fluently.\"\n",
    "\n",
    "To predict \"fluently\", the model needs to remember \"France\" — which occurred much earlier. LSTMs are capable of capturing such long-term dependencies.\n",
    "\n",
    "## Applications of LSTMs in NLP\n",
    "\n",
    "LSTMs have been successfully applied to:\n",
    "- **Text Generation**\n",
    "- **Sentiment Analysis**\n",
    "- **Machine Translation**\n",
    "- **Speech Recognition**\n",
    "\n",
    "### Example in Text Generation:\n",
    "\n",
    "Feed in: \"Once upon a\"\n",
    "\n",
    "→ Model completes: \"Once upon a time there was a king.\"\n",
    "\n",
    "The model learns patterns in language and generates coherent text.\n",
    "\n",
    "## LSTMs vs Simple RNNs\n",
    "\n",
    "LSTMs outperform simple RNNs on most NLP tasks because of their ability to:\n",
    "- Maintain relevant information over long distances.\n",
    "- Handle long-term dependencies without forgetting.\n",
    "- Converge faster during training due to better gradient flow.\n",
    "\n",
    "## Evolution: From LSTM to Transformers\n",
    "\n",
    "While LSTMs were dominant in the 2010s, newer models like **Transformers** (e.g., BERT, GPT) have now surpassed them in both performance and popularity.\n",
    "\n",
    "Transformers:\n",
    "- Don’t rely on recurrence.\n",
    "- Can be trained in parallel.\n",
    "- Are more scalable and efficient.\n",
    "\n",
    "Still, understanding RNNs and LSTMs is crucial as they form the foundation of modern NLP architectures.\n",
    "\n",
    "---\n",
    "\n",
    " **Remember:**  \n",
    "- Use RNNs for sequential data.  \n",
    "- Use LSTMs if you need to remember information over long time periods.  \n",
    "- For cutting-edge results, explore Transformers — the future of NLP.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68826f6-454a-4213-b7bf-58adf54a528a",
   "metadata": {},
   "source": [
    "# DAY 6 (task2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb48e4-e8a0-41d1-83f1-3f64539634d5",
   "metadata": {},
   "source": [
    "##  Stemming in NLP\n",
    "\n",
    "**Stemming** is a text normalization technique used to reduce words to their root or base form (called a \"stem\"). It's a rule-based process that chops off word suffixes. \n",
    "\n",
    "For example:\n",
    "- \"running\", \"runner\", and \"ran\" → \"run\"\n",
    "\n",
    "### Why Use Stemming?\n",
    "Stemming helps reduce vocabulary size in NLP tasks like text classification, sentiment analysis, and information retrieval, making models faster and less memory-intensive.\n",
    "\n",
    "### Popular Stemmers in NLTK:\n",
    "- **PorterStemmer**: Conservative and commonly used, developed by Martin Porter.\n",
    "- **LancasterStemmer**: More aggressive, might reduce words too much (over-stemming).\n",
    "\n",
    "In practice, stemming may result in non-words (e.g., \"happiness\" → \"happi\") but still retains the core meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e7edcb0-fb25-4500-a06b-c8ad1cdcb3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d44236ae-c4ff-4b80-9a58-bec3a843252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of words to stem\n",
    "words = [\"running\", \"runner\", \"easily\", \"fairly\", \"happiness\", \"crying\", \"flies\", \"flying\", \"denied\", \"studies\", \"studying\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12e030fc-4ba3-4fea-b549-c24eafe50b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original     PorterStemmer   LancasterStemmer\n",
      "---------------------------------------------\n",
      "running      run             run\n",
      "runner       runner          run\n",
      "easily       easili          easy\n",
      "fairly       fairli          fair\n",
      "happiness    happi           happy\n",
      "crying       cri             cry\n",
      "flies        fli             fli\n",
      "flying       fli             fly\n",
      "denied       deni            deny\n",
      "studies      studi           study\n",
      "studying     studi           study\n"
     ]
    }
   ],
   "source": [
    "# Create a table showing how each word changes after stemming\n",
    "print(f\"{'Original':<12} {'PorterStemmer':<15} {'LancasterStemmer'}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for word in words:\n",
    "    porter_stemmed = porter.stem(word)\n",
    "    lancaster_stemmed = lancaster.stem(word)\n",
    "    print(f\"{word:<12} {porter_stemmed:<15} {lancaster_stemmed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d068dccd-2262-45cd-b88f-18007f4300ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>LancasterStemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runner</td>\n",
       "      <td>runner</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>easily</td>\n",
       "      <td>easili</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fairly</td>\n",
       "      <td>fairli</td>\n",
       "      <td>fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happiness</td>\n",
       "      <td>happi</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>crying</td>\n",
       "      <td>cri</td>\n",
       "      <td>cry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flying</td>\n",
       "      <td>fli</td>\n",
       "      <td>fly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "      <td>deny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>studies</td>\n",
       "      <td>studi</td>\n",
       "      <td>study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>studying</td>\n",
       "      <td>studi</td>\n",
       "      <td>study</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Original PorterStemmer LancasterStemmer\n",
       "0     running           run              run\n",
       "1      runner        runner              run\n",
       "2      easily        easili             easy\n",
       "3      fairly        fairli             fair\n",
       "4   happiness         happi            happy\n",
       "5      crying           cri              cry\n",
       "6       flies           fli              fli\n",
       "7      flying           fli              fly\n",
       "8      denied          deni             deny\n",
       "9     studies         studi            study\n",
       "10   studying         studi            study"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df = pd.DataFrame({\n",
    "    \"Original\": words,\n",
    "    \"PorterStemmer\": [porter.stem(word) for word in words],\n",
    "    \"LancasterStemmer\": [lancaster.stem(word) for word in words]\n",
    "})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5bea7-ef0f-4013-bfac-1e1cf7ecbdf9",
   "metadata": {},
   "source": [
    "##  Lemmatization in NLP\n",
    "\n",
    "**Lemmatization** is a process of reducing words to their base or dictionary form (called a *lemma*), using context and vocabulary. Unlike stemming, it always returns valid words.\n",
    "\n",
    "For example:\n",
    "- \"running\" → \"run\"\n",
    "- \"better\" → \"good\"\n",
    "- \"was\" → \"be\"\n",
    "\n",
    "### Why Use Lemmatization?\n",
    "- More accurate than stemming (uses grammar rules and vocabulary)\n",
    "- Helps in tasks where understanding actual word forms is important, like language modeling, question answering, or chatbot systems.\n",
    "\n",
    "### How It Works:\n",
    "Lemmatization considers:\n",
    "- The word's **Part of Speech (POS)** (e.g., noun, verb, adjective)\n",
    "- Context from surrounding words\n",
    "\n",
    "### NLTK's Lemmatizer:\n",
    "- `WordNetLemmatizer` (requires POS tag for better accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63bbc73f-4959-4362-b049-b6b1bc698f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a0ded9c-670f-4da5-87a8-d0899f8cd812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "019c5c22-ac23-4d90-9ef0-207b3462c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged]\n",
    "    return ' '.join(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d42cf4a-e39e-4272-a26d-70d8071febde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : The striped bats are hanging on their feet for best.\n",
      "Lemmatized: The striped bat be hang on their foot for best .\n",
      "\n",
      "Original : He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\n",
      "Lemmatized: He be run and eat at same time . He have bad habit of swimming after play long hour in the Sun .\n",
      "\n",
      "Original : Cats chasing mice is a classic example in NLP.\n",
      "Lemmatized: Cats chase mouse be a classic example in NLP .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The striped bats are hanging on their feet for best.\",\n",
    "    \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\",\n",
    "    \"Cats chasing mice is a classic example in NLP.\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    print(f\"Original : {s}\")\n",
    "    print(f\"Lemmatized: {lemmatize_sentence(s)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdd6a9-f7ac-461e-ba88-1fe1da637772",
   "metadata": {},
   "source": [
    "# Day 7: Machine Translation – From Rule-Based to Neural Networks (task1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed513fe-7723-48fb-ac79-74ec2766ca0d",
   "metadata": {},
   "source": [
    "## Overview of Machine Translation (MT)\n",
    "Machine Translation (MT) refers to the use of algorithms to automatically translate text between human languages. The field has evolved through several phases:\n",
    "\n",
    "Rule-Based MT (RBMT) – Manually crafted rules and dictionaries.\n",
    "\n",
    "Statistical MT (SMT) – Data-driven models using phrase alignments.\n",
    "\n",
    "Neural MT (NMT) – End-to-end deep learning systems using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec55eb-0e6c-44ca-b2fa-187d84a0aebe",
   "metadata": {},
   "source": [
    "## Rule-Based Machine Translation\n",
    "Utilized linguistic rules: syntax trees, morphology, and bilingual dictionaries.\n",
    "\n",
    "Failed in real-world applications due to the complexity and ambiguity of natural languages.\n",
    "\n",
    " Limitation: Poor handling of polysemy and context (e.g., \"bass\" = fish or musical tone).\n",
    "\n",
    "## Statistical Machine Translation (SMT)\n",
    "Based on probabilistic models: 𝑃(𝐸∣𝐹)∝𝑃(𝐹∣𝐸)𝑃(𝐸) \n",
    "P(E∣F)∝P(F∣E)P(E)\n",
    "\n",
    "Translation model 𝑃(𝐹∣𝐸)\n",
    "P(F∣E): likelihood of source sentence F given target sentence E.\n",
    "\n",
    "Language model 𝑃(𝐸)\n",
    "P(E): fluency of target sentence.\n",
    "\n",
    "Learned from parallel corpora using word/phrase alignment algorithms (e.g., IBM Models, GIZA++).\n",
    "\n",
    " Example:\n",
    "Alignment of \"I eat apples\" → \"Je mange des pommes\" learned via word co-occurrence statistics.\n",
    "\n",
    "## Neural Machine Translation (NMT)\n",
    "Introduced around 2014, NMT uses sequence-to-sequence (seq2seq) models:\n",
    "\n",
    "Encoder: Converts input sequence into a dense vector.\n",
    "\n",
    "Decoder: Generates the output sequence from this vector.\n",
    "\n",
    "These models are trained end-to-end using cross-entropy loss on large parallel corpora.\n",
    "\n",
    "NMT better handles sentences like:\n",
    "Input: \"He didn’t turn up\"\n",
    "NMT: \"Il ne s’est pas présenté\" (correct idiomatic translation in French)\n",
    "\n",
    "\n",
    "## Sequence-to-Sequence (Seq2Seq) with RNNs\n",
    "Encoder and decoder are often implemented using LSTMs or GRUs.\n",
    "\n",
    "The encoder outputs a context vector, which summarizes the input.\n",
    "\n",
    "The decoder generates tokens one by one, optionally using teacher forcing during training (i.e., feeding the ground truth token at each time step).\n",
    "\n",
    " Limitation: Bottleneck caused by compressing long sequences into a single context vector.\n",
    "\n",
    "## Attention Mechanism\n",
    "To overcome the context bottleneck, attention allows the decoder to access the encoder’s hidden states at every time step:\n",
    "\n",
    "This dynamically weights the encoder outputs and improves long-sequence translation and alignment quality.\n",
    "\n",
    "## BLEU Score (Bilingual Evaluation Understudy)\n",
    "Evaluates translation quality based on n-gram precision against human references.\n",
    "\n",
    "Incorporates a brevity penalty to penalize short translations.\n",
    "\n",
    "Limitations: Doesn't measure semantic adequacy or fluency directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db964b-be8f-4b3d-a73f-35796fa786bd",
   "metadata": {},
   "source": [
    "# DAY 7 (task 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e1e8a-088a-490b-aadf-6a53480582b9",
   "metadata": {},
   "source": [
    "## Lemmatization: spaCy vs NLTK\n",
    "\n",
    "Lemmatization reduces a word to its base or dictionary form (called a lemma). Unlike stemming, it produces actual words and considers the part of speech, making it more accurate.\n",
    "\n",
    "We'll use:\n",
    "- **spaCy**: `.lemma_` property on tokens\n",
    "- **NLTK**: `WordNetLemmatizer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac8eb825-99b0-4ee0-9c25-e057457f8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f01de569-fce8-46f9-b913-e7dcd271d2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Lemmatization:\n",
      "The --> the\n",
      "striped --> striped\n",
      "bats --> bat\n",
      "were --> be\n",
      "hanging --> hang\n",
      "on --> on\n",
      "their --> their\n",
      "feet --> foot\n",
      "for --> for\n",
      "best --> good\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "sentence = \"The striped bats were hanging on their feet for best\"\n",
    "\n",
    "# spaCy lemmatization\n",
    "doc = nlp(sentence)\n",
    "print(\"spaCy Lemmatization:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} --> {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b880a656-d90f-45dd-8d74-945308320da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK Lemmatization:\n",
      "The --> The\n",
      "striped --> striped\n",
      "bats --> bat\n",
      "were --> were\n",
      "hanging --> hanging\n",
      "on --> on\n",
      "their --> their\n",
      "feet --> foot\n",
      "for --> for\n",
      "best --> best\n"
     ]
    }
   ],
   "source": [
    "# NLTK Lemmatization (default POS = noun)\n",
    "print(\"\\nNLTK Lemmatization:\")\n",
    "for word in sentence.split():\n",
    "    print(f\"{word} --> {lemmatizer.lemmatize(word)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a74a4-48eb-463a-8a0b-9b9a640cd463",
   "metadata": {},
   "source": [
    "## Stemming vs Lemmatization\n",
    "\n",
    "- **Stemming** is a rule-based approach that chops off word endings. It can result in non-words.\n",
    "- **Lemmatization** uses vocabulary and POS to return dictionary forms.\n",
    "\n",
    "Let's compare them using NLTK's PorterStemmer and WordNetLemmatizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3383cd3-2b9c-4f24-8341-291b7bc55efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            Stem            Lemma          \n",
      "=============================================\n",
      "running         run             running        \n",
      "flies           fli             fly            \n",
      "better          better          better         \n",
      "caring          care            caring         \n",
      "flying          fli             flying         \n",
      "denied          deni            denied         \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"flies\", \"better\", \"caring\", \"flying\", \"denied\"]\n",
    "\n",
    "print(\"{:<15} {:<15} {:<15}\".format(\"Word\", \"Stem\", \"Lemma\"))\n",
    "print(\"=\"*45)\n",
    "for word in words:\n",
    "    print(\"{:<15} {:<15} {:<15}\".format(word, stemmer.stem(word), lemmatizer.lemmatize(word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213c411-f7b1-41b6-8086-180829f5f538",
   "metadata": {},
   "source": [
    "## Why Lemmatization is Better\n",
    "\n",
    "- Stemming may convert \"caring\" → \"car\", which is misleading.\n",
    "- Lemmatization preserves meaning and returns valid dictionary words.\n",
    "- For tasks like text classification or search, lemmatization ensures semantic integrity.\n",
    "\n",
    "Example:\n",
    "- \"better\" → Lemma: \"good\"\n",
    "- \"flies\" → Lemma: \"fly\"\n",
    "- \"running\" → Lemma: \"run\"\n",
    "\n",
    "This allows better generalization of language patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19ce01-ae3d-4c9a-b793-9eabbb43aa3d",
   "metadata": {},
   "source": [
    "## Lemmatization in Preprocessing\n",
    "\n",
    "Let's add lemmatization as a step in a text preprocessing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e750efce-f9b2-4e6a-8ff8-8c161a78c1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The striped bats were hanging on their feet for best results.\n",
      "Processed: ['striped', 'bat', 'hang', 'foot', 'good', 'result']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())  # lowercase + tokenize\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in string.punctuation and not token.is_stop]\n",
    "    return tokens\n",
    "\n",
    "text = \"The striped bats were hanging on their feet for best results.\"\n",
    "print(\"Original:\", text)\n",
    "print(\"Processed:\", preprocess(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24343d48-e260-4ef4-aa33-5ae7aaac198a",
   "metadata": {},
   "source": [
    "# DAY 8 (task 1) : TextPreprocessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b953b41-2b34-4011-b1d5-edae59e7d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(self, tag):\n",
    "        \"\"\"Convert POS tag to a format compatible with WordNetLemmatizer\"\"\"\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        # POS tagging\n",
    "        tagged = pos_tag(tokens)\n",
    "        # Lemmatize\n",
    "        lemmatized = [self.lemmatizer.lemmatize(word, self.get_wordnet_pos(tag)) for word, tag in tagged]\n",
    "        return lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13af3e9f-2485-439d-b54f-297f031f8121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original 1: Cats are running faster than dogs in the park.\n",
      "Cleaned 1: ['cat', 'run', 'faster', 'dog', 'park']\n",
      "\n",
      "Original 2: He was driving the car very recklessly.\n",
      "Cleaned 2: ['drive', 'car', 'recklessly']\n",
      "\n",
      "Original 3: They had been eating at the new restaurant every weekend.\n",
      "Cleaned 3: ['eat', 'new', 'restaurant', 'every', 'weekend']\n"
     ]
    }
   ],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "texts = [\n",
    "    \"Cats are running faster than dogs in the park.\",\n",
    "    \"He was driving the car very recklessly.\",\n",
    "    \"They had been eating at the new restaurant every weekend.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(texts, 1):\n",
    "    print(f\"\\nOriginal {i}: {text}\")\n",
    "    print(f\"Cleaned {i}: {preprocessor.preprocess(text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8952afd3-5f46-4efa-aad7-0a7d95dcc49f",
   "metadata": {},
   "source": [
    "## spaCy Preprocessing Pipeline\n",
    "\n",
    "We’ll write a preprocessing function using spaCy to do:\n",
    "\n",
    "Lowercasing\n",
    "\n",
    "Tokenization\n",
    "\n",
    "Punctuation removal\n",
    "\n",
    "Stopword removal\n",
    "\n",
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f0c39bb-1579-455a-a772-40fdf15d6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    clean_tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space and not token.is_stop:\n",
    "            clean_tokens.append(token.lemma_.lower())\n",
    "    return clean_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8a7446e-bb60-4f46-a962-4ec6c527af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Output 1: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'park']\n",
      "spaCy Output 2: ['natural', 'language', 'processing', 'spacy', 'efficient', 'accurate']\n",
      "NLTK Output 1: ['quick', 'brown', 'fox', 'jumping', 'lazy', 'dog', 'park']\n",
      "NLTK Output 2: ['natural', 'language', 'processing', 'spacy', 'quite', 'efficient', 'accurate']\n"
     ]
    }
   ],
   "source": [
    "sample_text1 = \"The quick brown foxes were jumping over the lazy dogs in the park.\"\n",
    "sample_text2 = \"Natural Language Processing with spaCy is quite efficient and accurate!\"\n",
    "\n",
    "# Test spaCy\n",
    "print(\"spaCy Output 1:\", spacy_preprocess(sample_text1))\n",
    "print(\"spaCy Output 2:\", spacy_preprocess(sample_text2))\n",
    "\n",
    "# Test NLTK\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def nltk_preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords + lemmatize\n",
    "    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return clean_tokens\n",
    "\n",
    "print(\"NLTK Output 1:\", nltk_preprocess(sample_text1))\n",
    "print(\"NLTK Output 2:\", nltk_preprocess(sample_text2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1536ab-68b9-473a-86a0-baf6072e5284",
   "metadata": {},
   "source": [
    "### Pros & Cons\n",
    "\n",
    "### spaCy\n",
    "\n",
    "Pros:\n",
    "\n",
    "More accurate lemmatization (context-aware)\n",
    "\n",
    "Built-in support for tokenization, POS tagging, NER\n",
    "\n",
    "Handles multi-word expressions, contractions better\n",
    "\n",
    "Cons:\n",
    "\n",
    "Heavier to load (~30–50MB)\n",
    "\n",
    "Requires installing large models for deep tasks\n",
    "\n",
    "### NLTK\n",
    "Pros:\n",
    "\n",
    "Lightweight and customizable\n",
    "\n",
    "Easy to integrate with other Python tools\n",
    "\n",
    "Good for educational purposes\n",
    "\n",
    "Cons:\n",
    "\n",
    "Lemmatizer is dictionary-based (WordNet), not context-aware\n",
    "\n",
    "Tokenization and POS tagging are simpler (less accurate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45440d-1ec9-49c8-b282-df3c526e6cfe",
   "metadata": {},
   "source": [
    "# DAY 8 (task 2) :  LEC8-Self-Attention and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aacbbb9-ec86-42b6-ad71-c1e52b512e8b",
   "metadata": {},
   "source": [
    "## 1. Why Do We Need Transformers?\n",
    "Traditional RNNs and LSTMs process text sequentially, which causes problems:\n",
    "\n",
    "Can't parallelize easily\n",
    "\n",
    "Hard to learn long-range dependencies\n",
    "\n",
    "Slow inference/training\n",
    "\n",
    "Transformers solve this by using self-attention, allowing the model to \"look at all words at once\" instead of step-by-step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7335a9-121a-4023-a2ac-52427361ece4",
   "metadata": {},
   "source": [
    "##  2. Self-Attention Intuition\n",
    "Each word tries to understand:\n",
    "\n",
    "\"Which other words in the sentence are important for me?\"\n",
    "\n",
    "So every word computes a weighted average of all words in the sequence.\n",
    "\n",
    "Let’s break this down with a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09b1f116-d781-4edb-a5d6-cf706da4734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac1f62-5e15-447c-a65c-3fafbb7053ce",
   "metadata": {},
   "source": [
    "##  3. Self-Attention Math: Queries, Keys, and Values\n",
    "Think of Self-Attention as a Lookup System\n",
    "In any sentence, when you read a word, you look around to understand its meaning based on context. Self-attention mimics this by letting each word look at other words and decide how much attention to pay to them.\n",
    "\n",
    "To do that, we create 3 vectors for each word:\n",
    "\n",
    "Query (Q): What I'm looking for\n",
    "\n",
    "Key (K): What I contain\n",
    "\n",
    "Value (V): What I offer if you pay attention to me\n",
    "\n",
    "Each word asks: “How much should I pay attention to every other word?”\n",
    "The answer depends on how well my Query matches the other word’s Key.\n",
    "\n",
    "Let’s take a toy example sentence:\n",
    "Sentence: \"The cat sat\"\n",
    "Let’s assume each word has an embedding vector of size 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0feddde5-910a-4cd1-b38f-7fed80b9c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      " [[0.51905558 0.83677799 0.91324065]\n",
      " [0.58315394 0.92929156 1.02900759]\n",
      " [0.50882941 0.82193301 0.89451367]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's pretend each word is a 3D embedding\n",
    "words = [\"The\", \"cat\", \"sat\"]\n",
    "word_vectors = np.random.rand(3, 3)  # (3 words, 3 dimensions)\n",
    "\n",
    "# Random weight matrices for query, key, and value\n",
    "W_q = np.random.rand(3, 3)\n",
    "W_k = np.random.rand(3, 3)\n",
    "W_v = np.random.rand(3, 3)\n",
    "\n",
    "Q = word_vectors @ W_q\n",
    "K = word_vectors @ W_k\n",
    "V = word_vectors @ W_v\n",
    "\n",
    "# Now Q, K, V are all shape (3, 3) — one row per word.\n",
    "# Attention scores = Q @ K.T\n",
    "attention_scores = Q @ K.T\n",
    "\n",
    "# This gives a 3x3 matrix:\n",
    "\n",
    "# Row i tells you how much word i (as query) attends to each word j (as key).\n",
    "\n",
    "# attention_scores[i][j] = dot(Q[i], K[j])\n",
    "# Apply softmax row-wise to normalize \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(attention_scores)\n",
    "\n",
    "# Final self-attention output\n",
    "output = attention_weights @ V\n",
    "\n",
    "print(\"Attention Output:\\n\", output)\n",
    "# So output[i] is the new embedding for word i, created by a weighted sum of all value\n",
    "# vectors (V) — based on how much attention it gives to each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01390e8-894f-4610-ac46-1cf6de1c1028",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding: Adding Order\n",
    "Transformers don't know word order by default (since they look at all words at once). So we add positional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "388206af-1407-4caa-8f48-5c07f7358c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_positional_encoding(seq_len, d_model):\n",
    "    PE = np.zeros((seq_len, d_model))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            PE[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                PE[pos, i+1] = np.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
    "    return PE\n",
    "\n",
    "PE = get_positional_encoding(10, 16)\n",
    "plt.imshow(PE.T, cmap=\"viridis\")\n",
    "plt.title(\"Positional Encoding\")\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Encoding Dimension\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ee02c-b673-4e9b-a721-c993ff930a30",
   "metadata": {},
   "source": [
    "##  5. Multi-Head Attention\n",
    "Instead of doing self-attention once, we do it in parallel heads and concatenate the results. This allows the model to learn different types of relationships."
   ]
  },
  {
   "cell_type": "raw",
   "id": "41227ffb-4f9d-4dd2-ba7c-9615e7462614",
   "metadata": {},
   "source": [
    "# Pseudocode (not executable here)\n",
    "for each head:\n",
    "    compute Q, K, V → attention → output\n",
    "\n",
    "concat all heads → project to final output space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028860c2-9349-4631-9954-82d1c6f957c8",
   "metadata": {},
   "source": [
    "## 6. Transformer Block\n",
    "Each block contains:\n",
    "\n",
    "Multi-head self-attention\n",
    "\n",
    "Add & LayerNorm\n",
    "\n",
    "Feed Forward (Dense) Layer\n",
    "\n",
    "Add & LayerNorm\n",
    "\n",
    "These are stacked multiple times in both encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313524d5-ec1e-4490-8c2a-5d6cbdf778a6",
   "metadata": {},
   "source": [
    "##  7. Decoder + Masked Attention\n",
    "In generation (like GPT), we mask future words so the model can’t cheat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb18738c-7e0e-4856-bd9d-84a3ef54db4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Mask = upper triangle matrix\n",
    "def get_mask(seq_len):\n",
    "    return np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "\n",
    "mask = get_mask(5)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcb9b4-1716-4bfc-ac03-d3d1b1bd1892",
   "metadata": {},
   "source": [
    "##  8. Why Transformers Work So Well\n",
    "Parallelizable training\n",
    "\n",
    "Global context for every word\n",
    "\n",
    "Effective long-range dependency capture\n",
    "\n",
    "Scales well with data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767401d1-dba7-4f6e-a9bf-bcf9c48d362f",
   "metadata": {},
   "source": [
    "## 9. Limitations\n",
    "Expensive for very long inputs\n",
    "\n",
    "Quadratic time & memory complexity\n",
    "\n",
    "Still an active research area (e.g., Longformer, FlashAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ceab1-a7d4-49b3-9ee8-3d474b663c80",
   "metadata": {},
   "source": [
    "# DAY 9 (task1) (30.5.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d761c1-69ee-453a-bc72-94ab79bb5b7d",
   "metadata": {},
   "source": [
    "## Run preprocessing pipeline on sample corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d34ec584-921a-4b63-a0f5-81caa598b41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Asus/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e45d4ac-dd86-412a-b9f3-366d875411e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define sample corpus\n",
    "# Use 2 sample movie reviews for now\n",
    "sample_docs = []\n",
    "for fileid in movie_reviews.fileids()[:2]:\n",
    "    sample_docs.append(movie_reviews.raw(fileid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6659575c-9b7e-4472-869a-14690aed1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb8f3b42-48c0-456e-bf7b-8e68e51dea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_docs = []\n",
    "for doc in sample_docs:\n",
    "    tokens = preprocess(doc)\n",
    "    preprocessed_docs.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "853f0429-e8e6-4d30-b9d3-f78d58b9c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f6e0e-532e-4b04-bcf0-0955f6f8e74b",
   "metadata": {},
   "source": [
    "## Visualize before vs. after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "586c05ac-28f8-493e-a5e9-2a120f3774bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt\n",
      "\n",
      "Cleaned Tokens:\n",
      " ['plot', 'two', 'teen', 'couple', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guy', 'dy', 'girlfriend', 'continues', 'see', 'life', 'nightmare', 'whats', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mindfuck', 'movie', 'teen', 'generation', 'touch', 'cool', 'idea', 'present', 'bad', 'package', 'make', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'film', 'attempt', 'break', 'mold', 'mess']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\\n\", sample_docs[0][:500])\n",
    "print(\"\\nCleaned Tokens:\\n\", preprocessed_docs[0][:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80018aa7-acce-477a-9d2a-7fd2a09161dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter(preprocessed_docs[0])\n",
    "most_common = word_freq.most_common(15)\n",
    "words, freqs = zip(*most_common)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, freqs)\n",
    "plt.title(\"Top 15 Words After Preprocessing\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b60c9-719c-467b-8c5e-1bee5493d6a4",
   "metadata": {},
   "source": [
    "# DAY 9 (task2): \n",
    "\n",
    "# CS224N Lecture: Pretraining in NLP\n",
    "\n",
    "---\n",
    "\n",
    "##  Introduction to Pretraining\n",
    "\n",
    "Pretraining refers to training a language model on a large, unlabeled corpus to learn general linguistic patterns before fine-tuning it for a specific task (e.g., sentiment analysis, translation).\n",
    "\n",
    "It enables models to:\n",
    "- Learn rich, contextual representations of words.\n",
    "- Generalize better across various NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "##  The Problem with Fixed Vocabularies\n",
    "\n",
    "Earlier models like Word2Vec had a fixed vocabulary. Words not in the vocabulary were replaced with an **UNK** (unknown) token, losing valuable semantic information.\n",
    "\n",
    "### Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1003d92a-27b0-4843-8e78-d4a538ed0bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK\n"
     ]
    }
   ],
   "source": [
    "sentence = \"unhappiness\"\n",
    "vocab = {\"happy\", \"sad\", \"joy\"}\n",
    "\n",
    "if sentence not in vocab:\n",
    "    print(\"UNK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9963e-b480-4117-ba1c-e5156b9de6fd",
   "metadata": {},
   "source": [
    "This is problematic because unhappiness clearly carries sentiment and meaning, but gets dropped entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd86a96-7a25-4546-a823-bc08ee041c1e",
   "metadata": {},
   "source": [
    "## Subword Modeling\n",
    "Instead of treating unhappiness as a single word, we break it into subwords like:\n",
    "\n",
    "un + happy + ness\n",
    "\n",
    "Subword tokenization (e.g., using BPE or WordPiece) helps model rare or unseen words more effectively.\n",
    "\n",
    "Example using Hugging Face Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7d0b38f-65af-427e-980c-c577f74f054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un', '##ha', '##pp', '##iness']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"unhappiness\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecbb4d-b854-4f37-96f9-a1a802c23207",
   "metadata": {},
   "source": [
    "## Contextual Representations\n",
    "Unlike Word2Vec (static embeddings), contextual models generate different vectors for the same word based on surrounding context.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ebf2ea7e-69eb-475f-80ac-a93340782f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68225a7f0404a93ad9fba1ef8bed31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different embeddings for 'bank' depending on context\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence1 = \"He went to the bank to deposit money\"\n",
    "sentence2 = \"She sat by the river bank\"\n",
    "\n",
    "inputs1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "inputs2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1).last_hidden_state\n",
    "    outputs2 = model(**inputs2).last_hidden_state\n",
    "\n",
    "print(\"Different embeddings for 'bank' depending on context\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62978fb1-9587-4d67-9eee-338a28c48ebd",
   "metadata": {},
   "source": [
    "## Pretraining Objectives\n",
    "1. Masked Language Modeling (MLM)\n",
    "Used in BERT. Randomly masks ~15% of tokens, and the model tries to predict them.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1b18526-95ec-4407-90c0-7059ce61cccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.16506372392177582,\n",
       "  'token': 2204,\n",
       "  'token_str': 'good',\n",
       "  'sequence': 'the weather is good today.'},\n",
       " {'score': 0.11934636533260345,\n",
       "  'token': 4010,\n",
       "  'token_str': 'warm',\n",
       "  'sequence': 'the weather is warm today.'},\n",
       " {'score': 0.060145191848278046,\n",
       "  'token': 10256,\n",
       "  'token_str': 'mild',\n",
       "  'sequence': 'the weather is mild today.'},\n",
       " {'score': 0.043130915611982346,\n",
       "  'token': 2986,\n",
       "  'token_str': 'fine',\n",
       "  'sequence': 'the weather is fine today.'},\n",
       " {'score': 0.03318683058023453,\n",
       "  'token': 4658,\n",
       "  'token_str': 'cool',\n",
       "  'sequence': 'the weather is cool today.'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "fill_mask(\"The weather is [MASK] today.\")\n",
    "\n",
    "#Output: [sunny, nice, good, rainy, warm] — based on context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c95f3-f537-4781-8c30-5a6cb07338be",
   "metadata": {},
   "source": [
    "## Model Architectures\n",
    "Encoder-Decoder\n",
    "Used in sequence-to-sequence tasks like translation.\n",
    "\n",
    "E.g., T5, BART.\n",
    "\n",
    "Decoder-Only\n",
    "Used in autoregressive language generation.\n",
    "\n",
    "E.g., GPT family (GPT-2, GPT-3, GPT-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97675739-2a38-423b-946b-eb99a95b7f04",
   "metadata": {},
   "source": [
    "## GPT and Autoregressive Pretraining\n",
    "GPT uses a decoder-only architecture and trains with causal language modeling:\n",
    "\n",
    "Predict the next word based on previous words.\n",
    "\n",
    "No masking; instead, future tokens are hidden during training.\n",
    "\n",
    "### Example: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4edc1f70-f195-493a-b435-dfe16a4b86c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603d17e9ae26462da5701168fc3cc6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a78ccb51d454330945e54da93f7286a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6582a18412034e85ae3f59149e78884c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32fad861ffd437bb16ff3403bf35394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47126caff26a430991f051fb9dce314c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4af3fd8a9814df4b446bc77e4e003fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd62f71b3e9d4092beeb2341a23a962e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The future of AI is uncertain, as AI remains a very nascent field. But it has been shown to be able to learn a lot, and to be able to solve a lot of problems, in a very short amount of time. It's an open question of whether or not human-machine interactions will ever solve the problems of AI.\\n\\nSo, with the technology that we've been able to create, I'm not sure that the future of AI will be that far-fetched. I'm not sure that it will be quite the opposite. There will be lots of problems that AI can solve, and lots of people will be able to solve them. But I'm not sure that AI will be a lot more difficult to solve than computer programs or artificial intelligence. That's the question that I'm not sure is so far-fetched at this point.\\n\\nThe question is how long will that be, and what can we expect to achieve with the technology that we're working with, and the possibilities for AI that we're working with.\\n\\nThe current technology has an enormous potential to solve most of the problems that we face. What we're seeing with the AI is that it's a very fast-growing field. It's getting bigger and bigger, and so far it has developed\"}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generator(\"The future of AI is\", max_length=20, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434b741-7f4e-4d84-9315-e25e3ce79ef1",
   "metadata": {},
   "source": [
    "## Improvements in Pretraining\n",
    "RoBERTa improved on BERT by removing the Next Sentence Prediction task and training longer on more data.\n",
    "\n",
    "Lightweight fine-tuning methods (e.g., adapters, LoRA) reduce training costs and memory use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d9b5fa-e930-4ea7-99fd-d6cb34d8c990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
