{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2185a866-e277-4283-9263-4b9136885a1a",
   "metadata": {},
   "source": [
    "# üóìÔ∏è June 19 ‚Äì Sequence Modeling Foundations\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Why Traditional Models Fail on Sequential Data\n",
    "\n",
    "Traditional machine learning models (like Logistic Regression, Naive Bayes, and SVMs) process **text as a bag of words** or fixed-length feature vectors.\n",
    "\n",
    "They ignore:\n",
    "- **Word order**\n",
    "- **Contextual meaning**\n",
    "- **Temporal dependencies**\n",
    "\n",
    "### ‚ùå Problem Example:\n",
    "Consider:\n",
    "- Sentence A: \"I did not enjoy the movie.\"\n",
    "- Sentence B: \"I enjoyed the movie.\"\n",
    "\n",
    "Both contain similar words, but **meaning is opposite**. Traditional models may assign them similar sentiment due to bag-of-words encoding.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are designed to handle sequences by **remembering previous inputs** using a hidden state that gets updated at each step.\n",
    "\n",
    "### üîÅ RNN Working:\n",
    "At each time step `t`, it uses:\n",
    "- Current input `x‚Çú`\n",
    "- Previous hidden state `h‚Çú‚Çã‚ÇÅ`\n",
    "\n",
    "To compute:\n",
    "```math\n",
    "h‚Çú = tanh(W‚Çìx‚Çú + W‚Çïh‚Çú‚Çã‚ÇÅ + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9464d88-6b6a-4da4-bb1d-7a77ecacc7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple RNN Computation:\n",
      "\n",
      "Time Step 1: Hidden State = [-0.49749747 -0.0036857  -0.39356345 -0.94412524  0.6471663 ]\n",
      "Time Step 2: Hidden State = [-0.93307126  0.5306133   0.95593184 -0.9828039   0.99930495]\n",
      "Time Step 3: Hidden State = [ 0.9712605   0.9459313   0.40194666 -0.99999994  0.97436684]\n"
     ]
    }
   ],
   "source": [
    "# Simple RNN from Scratch\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simulated input: 3 time steps, 4 input features\n",
    "x_seq = torch.randn(3, 4)\n",
    "hidden_dim = 5\n",
    "h_prev = torch.zeros(hidden_dim)\n",
    "\n",
    "# Weights\n",
    "Wx = torch.randn(4, hidden_dim)\n",
    "Wh = torch.randn(hidden_dim, hidden_dim)\n",
    "b = torch.randn(hidden_dim)\n",
    "\n",
    "# RNN Loop\n",
    "print(\"Simple RNN Computation:\\n\")\n",
    "for t in range(3):\n",
    "    x_t = x_seq[t]\n",
    "    h_prev = torch.tanh(x_t @ Wx + h_prev @ Wh + b)\n",
    "    print(f\"Time Step {t+1}: Hidden State = {h_prev.detach().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c973f9e1-c985-4235-bd44-9d12506a155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "# Built-in RNN Layer in PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# A simple RNN layer\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, batch_first=True)\n",
    "\n",
    "#This creates a vanilla RNN layer with the following:\n",
    "\n",
    "#input_size=10: Each time step has 10 features.\n",
    "#hidden_size=20: The RNN will output a hidden vector of size 20.\n",
    "#batch_first=True: Input/output tensors will have shape (batch, seq_len, feature).\n",
    "#This means your input and output tensors will look like (batch_size, sequence_length, input_size or hidden_size).\n",
    "\n",
    "# Input: batch_size x seq_len x input_size\n",
    "x = torch.randn(5, 3, 10)\n",
    "#Creates a random input tensor:\n",
    "\n",
    "#batch_size = 5: 5 different sequences (like 5 sentences)\n",
    "#sequence_length = 3: Each sequence has 3 time steps (like 3 words)\n",
    "#input_size = 10: Each time step is represented by a 10-dimensional vector (like a word embedding)\n",
    "#x shape = [5, 3, 10]\n",
    "\n",
    "h0 = torch.zeros(1, 5, 20)  # (num_layers, batch, hidden_size)\n",
    "#Initial hidden state:\n",
    "#1 ‚Üí Number of RNN layers (num_layers)\n",
    "#5 ‚Üí Batch size (same as input)\n",
    "#20 ‚Üí Hidden size (output feature size)\n",
    "#h0 shape = [num_layers, batch_size, hidden_size]\n",
    "\n",
    "out, hn = rnn(x, h0)\n",
    "#Passes the input x and initial hidden state h0 into the RNN.\n",
    "#Returns:\n",
    "#out: Hidden states at each time step for each sequence ‚Üí shape [batch_size, seq_len, hidden_size]\n",
    "#hn: Hidden state only at the final time step ‚Üí shape [num_layers, batch_size, hidden_size]\n",
    "#So:\n",
    "#out.shape = [5, 3, 20]\n",
    "#hn.shape = [1, 5, 20]\n",
    "\n",
    "print(\"Output shape:\", out.shape)  # (batch_size, seq_len, hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63403daa-d970-480a-9dc5-63b6e5403c90",
   "metadata": {},
   "source": [
    "# Advantages of RNNs\n",
    "‚úÖ Learns from arbitrary-length sequences\n",
    "\n",
    "‚úÖ Shared weights across time steps\n",
    "\n",
    "‚úÖ Can model sequential dependencies (to some extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15cf7d-3759-41de-a679-8dcdea10b1d8",
   "metadata": {},
   "source": [
    "# Limitations of RNNs\n",
    "‚ùå Sequential computation ‚Üí slow to train\n",
    "\n",
    "‚ùå Short-term memory ‚Üí loses info over long sequences\n",
    "\n",
    "‚ùå Suffers from vanishing gradients when backpropagating over time\n",
    "\n",
    "These limitations led to improved models: LSTMs and GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec02830-5f61-462a-8c1d-7fa5cd375e0a",
   "metadata": {},
   "source": [
    "# 3. LSTM ‚Äì Long Short-Term Memory (Preview)\n",
    "LSTMs improve upon RNNs by using:\n",
    "\n",
    "A cell state to preserve long-term information\n",
    "\n",
    "Gates to decide what to keep, forget, and output\n",
    "\n",
    "They are designed to overcome vanishing gradients and capture long-range dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ad2ff-dc06-4fc1-87ee-a26a6c679e04",
   "metadata": {},
   "source": [
    "# Extra: From Neural Dependency Parsing Lecture\n",
    "Neural parsers use dense, learned representations instead of symbolic grammar rules.\n",
    "\n",
    "RNNs (and later LSTMs) help in modeling dependencies in sentences efficiently and accurately.\n",
    "\n",
    "These architectures form the base for sequence models in modern NLP.\n",
    "\n",
    "# üìå Recap\n",
    "Concept\tSummary\n",
    "\n",
    "Traditional Models     | \tLose order/context\n",
    "\n",
    "RNNs\t               |   Learn from sequences\n",
    "\n",
    "PyTorch RNN\t           |  Easy implementation with nn.RNN\n",
    "\n",
    "Limitations\t           |   Vanishing gradients, slow training\n",
    "\n",
    "Solution Preview\t   |        LSTMs, GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b9688-95c8-4d7f-8610-72a85c08cfe6",
   "metadata": {},
   "source": [
    "# üß† June 20‚Äì21 ‚Äì LSTMs, Bidirectional LSTMs, and Attention Mechanisms\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Language Modeling and RNNs in NLP\n",
    "\n",
    "### üß† Language Modeling Task:\n",
    "Given a sequence of words, predict the next word:\n",
    "> Input: \"I am going to the\" ‚Üí Predict: \"store\"\n",
    "\n",
    "### ‚ùå Traditional models (e.g., n-grams) are limited by:\n",
    "- Fixed context window\n",
    "- Inability to generalize across similar patterns\n",
    "\n",
    "### ‚úÖ RNNs improve by:\n",
    "- Maintaining a **hidden state** that captures prior context\n",
    "- Learning dependencies from previous words\n",
    "\n",
    "---\n",
    "\n",
    "## üîß RNN in Language Modeling ‚Äì Code Recap\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# RNN layer\n",
    "rnn = nn.RNN(input_size=10, hidden_size=20, batch_first=True)\n",
    "\n",
    "x = torch.randn(5, 3, 10)        # batch_size=5, seq_len=3, input_dim=10\n",
    "h0 = torch.zeros(1, 5, 20)       # num_layers=1, batch_size=5, hidden_dim=20\n",
    "\n",
    "out, hn = rnn(x, h0)\n",
    "print(out.shape)  # (5, 3, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1f725-926e-44e0-b0fb-113b1f1d129b",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è 2. Challenges in RNNs\n",
    "### ‚ùå Vanishing Gradients\n",
    "As the sequence grows, gradients become too small during backpropagation.\n",
    "\n",
    "Model \"forgets\" earlier parts of long sequences.\n",
    "\n",
    "üß™ Example:\n",
    "Sentence: \"The movie was not good.\"\n",
    "RNN might remember only \"good\", forgetting \"not\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37841a4b-3db4-4fb3-b488-254d2c907e57",
   "metadata": {},
   "source": [
    "# üîπ 3. Long Short-Term Memory (LSTM) Networks\n",
    "LSTM solves vanishing gradients via gates that control memory flow:\n",
    "\n",
    "Forget Gate ‚Äì What to discard\n",
    "\n",
    "Input Gate ‚Äì What to store\n",
    "\n",
    "Output Gate ‚Äì What to output\n",
    "\n",
    "## üß† Example:\n",
    "Input: \"I lived in France for two years, so I speak French fluently.\"\n",
    "LSTM can retain \"France\" till \"fluently\" due to gated memory control.\n",
    "\n",
    "## üîß Code: Simple LSTM Model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed9476b-519a-4e5f-8e8f-73155c1ee8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(input_size=10, hidden_size=20, batch_first=True)\n",
    "\n",
    "x = torch.randn(5, 3, 10)         # batch_size=5, seq_len=3\n",
    "h0 = torch.zeros(1, 5, 20)        # (num_layers, batch_size, hidden_dim)\n",
    "c0 = torch.zeros(1, 5, 20)        # cell state\n",
    "\n",
    "out, (hn, cn) = lstm(x, (h0, c0))\n",
    "\n",
    "print(out.shape)   # Shape: (5, 3, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d50f01-63c3-4cfd-a266-6db420ba3df3",
   "metadata": {},
   "source": [
    "# üîπ 4. Bidirectional LSTM\n",
    "BiLSTM reads the sequence forward and backward, capturing both past and future context.\n",
    "\n",
    "## ‚úÖ Useful for:\n",
    "Named Entity Recognition\n",
    "\n",
    "Sentiment Analysis\n",
    "\n",
    "Any task needing context from both sides\n",
    "\n",
    "## üîß Code: BiLSTM in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2407054-b297-4047-9472-9848ff12a9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 40])\n"
     ]
    }
   ],
   "source": [
    "bilstm = nn.LSTM(input_size=10, hidden_size=20, bidirectional=True, batch_first=True)\n",
    "\n",
    "x = torch.randn(5, 3, 10)\n",
    "h0 = torch.zeros(2, 5, 20)  # 2 for bidirectional\n",
    "c0 = torch.zeros(2, 5, 20)\n",
    "\n",
    "out, (hn, cn) = bilstm(x, (h0, c0))\n",
    "print(out.shape)  # Shape: (5, 3, 40) ‚Üí 20 for forward + 20 for backward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d8822-7f5f-465c-853c-588bdf4bcf0d",
   "metadata": {},
   "source": [
    "# üîπ 5. Attention Mechanism (June 21)\n",
    "RNNs (even LSTMs) may still struggle with very long sequences.\n",
    "\n",
    "## üß† Attention helps by:\n",
    "Learning which parts of the sequence are important\n",
    "\n",
    "Computing a weighted average of all hidden states\n",
    "\n",
    "## üîß Custom Attention Layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab154cb-3c4c-46b3-9f76-437a15f2b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        # lstm_out: [batch, seq_len, hidden_dim]\n",
    "        scores = self.attn(lstm_out).squeeze(-1)         # [batch, seq_len]\n",
    "        weights = torch.softmax(scores, dim=1)           # [batch, seq_len]\n",
    "        context = torch.sum(lstm_out * weights.unsqueeze(-1), dim=1)  # [batch, hidden_dim]\n",
    "        return context, weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea276b55-f828-4317-9cb1-01dbb8fdb2cc",
   "metadata": {},
   "source": [
    " ## Combine: BiLSTM + Attention Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730376d0-163c-40da-818f-d6f0bd477421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.attn = Attention(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        context, _ = self.attn(lstm_out)\n",
    "        return self.fc(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbdfec-0b2b-4a63-9a8e-d6beb9c14c4e",
   "metadata": {},
   "source": [
    "# üìä 6. Applications of LSTM in NLP\n",
    "| Task                | Example                               |\n",
    "| ------------------- | ------------------------------------- |\n",
    "| Language Modeling   | Predict next word                     |\n",
    "| Sentiment Analysis  | Positive or negative sentence         |\n",
    "| Text Generation     | \"Once upon a\" ‚Üí \"time there was a...\" |\n",
    "| Machine Translation | English ‚Üí French                      |\n",
    "| Speech Recognition  | Audio to text                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe7fb5-7672-4955-9c40-2847efbe5392",
   "metadata": {},
   "source": [
    "# üîÑ 7. RNNs vs. LSTMs vs. Transformers\n",
    "\n",
    "| Feature                | RNN   | LSTM | Transformer |\n",
    "| ---------------------- | ----- | ---- | ----------- |\n",
    "| Memory of past         | Short | Long | Global      |\n",
    "| Parallelizable         | ‚ùå    | ‚ùå  | ‚úÖ          |\n",
    "| Handles long sequences | ‚ùå    | ‚úÖ  | ‚úÖ          |\n",
    "| SOTA Performance       | ‚ùå    | ‚úÖ  | ‚úÖ‚úÖ       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99692f-995b-457a-a536-a86a6c0fe368",
   "metadata": {},
   "source": [
    "| Concept        | Summary                                          |\n",
    "| -------------- | ------------------------------------------------ |\n",
    "| RNN            | Sequence model with short-term memory            |\n",
    "| LSTM           | Handles long-term dependencies via gates         |\n",
    "| BiLSTM         | Reads input forwards and backwards               |\n",
    "| Attention      | Learns to focus on important parts of the input  |\n",
    "| Real-world Use | Sentiment analysis, text generation, translation |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299139ec-14c4-4d9c-b55c-3cb4d177327f",
   "metadata": {},
   "source": [
    "# üß† June 22‚Äì23 ‚Äì GRUs, Custom Data Handling, and Text Classification\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ June 22 ‚Äì GRUs and Data Handling\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Gated Recurrent Units (GRUs)\n",
    "\n",
    "GRUs are a simplified version of LSTMs with **fewer gates**, making them faster and easier to train while still solving the vanishing gradient problem.\n",
    "\n",
    "### üéØ Key Differences (vs LSTM):\n",
    "\n",
    "| Feature       | LSTM              | GRU               |\n",
    "|---------------|-------------------|-------------------|\n",
    "| Gates         | 3 (Forget, Input, Output) | 2 (Update, Reset) |\n",
    "| Cell State    | Separate cell and hidden state | Combined |\n",
    "| Complexity    | Higher            | Lower             |\n",
    "| Performance   | Similar (GRUs may perform better on smaller datasets) |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† GRU Equations\n",
    "\n",
    "```math\n",
    "z‚Çú = œÉ(Wz¬∑x‚Çú + Uz¬∑h‚Çú‚Çã‚ÇÅ)      ‚Üê Update Gate  \n",
    "r‚Çú = œÉ(Wr¬∑x‚Çú + Ur¬∑h‚Çú‚Çã‚ÇÅ)      ‚Üê Reset Gate  \n",
    "hÃÉ‚Çú = tanh(W¬∑x‚Çú + U¬∑(r‚Çú * h‚Çú‚Çã‚ÇÅ))  \n",
    "h‚Çú = (1 - z‚Çú) * h‚Çú‚Çã‚ÇÅ + z‚Çú * hÃÉ‚Çú\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a020c9-c68f-459f-8cd4-5fb3a116933e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "gru = nn.GRU(input_size=10, hidden_size=20, batch_first=True)\n",
    "\n",
    "x = torch.randn(5, 3, 10)      # batch_size = 5, seq_len = 3, input_size = 10\n",
    "h0 = torch.zeros(1, 5, 20)     # num_layers = 1, batch_size = 5, hidden_size = 20\n",
    "\n",
    "out, hn = gru(x, h0)\n",
    "print(out.shape)  # torch.Size([5, 3, 20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb65cc7-87e5-4d30-afaa-71b3252e5a0c",
   "metadata": {},
   "source": [
    "## 2. Custom Data Handling with torchtext\n",
    "We'll use torchtext to load and process the IMDb sentiment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2fd8f41-32a1-4f65-a92c-f4bac2b9265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Load training dataset\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "# Tokenize + build vocab\n",
    "def yield_tokens(data_iter):\n",
    "    for label, line in data_iter:\n",
    "        yield tokenizer(line)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f6848-d05b-4c37-b37d-7d2acb9762aa",
   "metadata": {},
   "source": [
    "üîÅ Encode Sentence Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ef1b91-f9da-4087-bf50-1925ae744ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['the', 'movie', 'was', 'great', '!']\n",
      "Encoded: [1, 20, 16, 92, 35]\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie was great!\"\n",
    "tokens = tokenizer(text)\n",
    "encoded = vocab(tokens)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Encoded:\", encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276db3d-36c7-4782-afb9-9ac86481de65",
   "metadata": {},
   "source": [
    "üß© Prepare Batches (basic version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dffccdcd-3059-4079-8717-dd4909df326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    labels, texts = [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(1 if label == \"pos\" else 0)\n",
    "        processed = torch.tensor(vocab(tokenizer(text)), dtype=torch.int64)\n",
    "        texts.append(processed)\n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    return torch.tensor(labels), texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66562e26-35e5-42ef-838f-a8e80090fba8",
   "metadata": {},
   "source": [
    "#üîπ June 23 ‚Äì Apply Models to Text Classification\n",
    "## 1. Sentiment Analysis Task: IMDb\n",
    "Binary classification task:\n",
    "\n",
    "pos ‚Üí 1\n",
    "\n",
    "neg ‚Üí 0\n",
    "\n",
    "## üß† Model Architecture (BiLSTM or GRU + FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1785e261-b345-4730-9066-8bfedf11385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        gru_out, _ = self.gru(x)\n",
    "        pooled = torch.mean(gru_out, dim=1)\n",
    "        return self.fc(pooled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc8156-b8ba-42fc-b78d-7e5677e94118",
   "metadata": {},
   "source": [
    "2. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f96f6b3-0684-49a1-b4b0-0e8ecb675487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for label, text in dataloader:\n",
    "            output = model(text)\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            preds.extend(pred.tolist())\n",
    "            labels.extend(label.tolist())\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    return acc, f1, cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acb1d0-9f3a-43b1-a78f-8d2a5893a67d",
   "metadata": {},
   "source": [
    "# ‚úÖ Summary\n",
    "| Task                  | Concept                              |\n",
    "| --------------------- | ------------------------------------ |\n",
    "| GRU                   | Efficient memory-based RNN           |\n",
    "| torchtext             | Handles tokenization, vocab, batches |\n",
    "| BiLSTM/GRU Classifier | Predicts sentiment                   |\n",
    "| Evaluation            | Accuracy, F1, Confusion Matrix       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cef08-3b4b-48f8-b3f6-88906431b09c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
